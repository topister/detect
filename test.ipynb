{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 13:20:27.204706: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-29 13:20:27.204747: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-29 13:20:27.206053: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-29 13:20:27.212931: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-29 13:20:28.121881: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in ./myenv/lib/python3.10/site-packages (3.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.10/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in ./myenv/lib/python3.10/site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./myenv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./myenv/lib/python3.10/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./myenv/lib/python3.10/site-packages (from matplotlib) (4.45.1)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in ./myenv/lib/python3.10/site-packages (from matplotlib) (1.26.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./myenv/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./myenv/lib/python3.10/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./myenv/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in ./myenv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in ./myenv/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./myenv/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in ./myenv/lib/python3.10/site-packages (from tensorflow) (2.15.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in ./myenv/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./myenv/lib/python3.10/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./myenv/lib/python3.10/site-packages (from tensorflow) (4.23.4)\n",
      "Requirement already satisfied: six>=1.12.0 in ./myenv/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: setuptools in ./myenv/lib/python3.10/site-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./myenv/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in ./myenv/lib/python3.10/site-packages (from tensorflow) (1.26.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./myenv/lib/python3.10/site-packages (from tensorflow) (4.8.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./myenv/lib/python3.10/site-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./myenv/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in ./myenv/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in ./myenv/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./myenv/lib/python3.10/site-packages (from tensorflow) (0.34.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./myenv/lib/python3.10/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./myenv/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./myenv/lib/python3.10/site-packages (from tensorflow) (1.59.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in ./myenv/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in ./myenv/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./myenv/lib/python3.10/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in ./myenv/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./myenv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./myenv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./myenv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.23.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./myenv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in ./myenv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./myenv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./myenv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./myenv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./myenv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./myenv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./myenv/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./myenv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in ./myenv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./myenv/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nvidia-tensorrt in ./myenv/lib/python3.10/site-packages (99.0.0)\n",
      "Requirement already satisfied: tensorrt in ./myenv/lib/python3.10/site-packages (from nvidia-tensorrt) (8.6.1.post1)\n"
     ]
    }
   ],
   "source": [
    "! pip install nvidia-tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.applications import VGG16\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Flatten, Dense, Dropout, Input\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "from keras.applications import MobileNetV2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GlobalAveragePooling2D, Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory paths\n",
    "\n",
    "train_dir = '/home/topister/Desktop/ML/GroupAssignment1/Transfer Learning/NEU Metal Surface Defects Data/train'\n",
    "valid_dir = '/home/topister/Desktop/ML/GroupAssignment1/Transfer Learning/NEU Metal Surface Defects Data/train'\n",
    "test_dir = '/home/topister/Desktop/ML/GroupAssignment1/Transfer Learning/NEU Metal Surface Defects Data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSES = 6       # There are 6 classes of defects in each directory: Crazing, Inclusion, Patches, Pitted, Rolled, Scratches\n",
    "# HEIGHT = 224      # Height of the input image for the model\n",
    "# WIDTH = 224       # Width of the input image for the model\n",
    "# CHANNELS = 3      # Number of color channels in the image (RGB - Red, Green, Blue)\n",
    "# BATCH_SIZE = 32   # Number of images processed in a single batch during training or inference\n",
    "# Epochs = 5\n",
    "\n",
    "\n",
    "HEIGHT = 224\n",
    "WIDTH = 224\n",
    "CHANNELS = 3\n",
    "CLASSES = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "baseModel = MobileNetV2(input_shape=(HEIGHT, WIDTH, CHANNELS),\n",
    "                        include_top=False,\n",
    "                        weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a VGG19 model pre-trained on ImageNet without including the top classification layers\n",
    "# baseModel = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(WIDTH, HEIGHT, CHANNELS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze base model layers\n",
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Enable Transfer Learning by freezing weights of the base VGG16 Model\n",
    "# for layer in baseModel.layers[:-4]:  # Unfreeze some layers towards the end\n",
    "#     layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    baseModel,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(CLASSES, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_224 (Func  (None, 7, 7, 1280)        2257984   \n",
      " tional)                                                         \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 1280)              0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               327936    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 1542      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2587462 (9.87 MB)\n",
      "Trainable params: 329478 (1.26 MB)\n",
      "Non-trainable params: 2257984 (8.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data augmentation and normalization for the training dataset\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,              # Rescale pixel values to [0,1]\n",
    "    rotation_range=20,           # Randomly rotate images within 20 degrees\n",
    "    shear_range=0.2,             # Apply shear transformation\n",
    "    zoom_range=0.2,              # Apply random zoom\n",
    "    horizontal_flip=True,        # Flip images horizontally\n",
    "    vertical_flip=True,           # Flip images vertically\n",
    "    fill_mode='nearest',\n",
    ")\n",
    "\n",
    "# Data augmentation and normalization for the validation dataset\n",
    "valid_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,              # Rescale pixel values to [0,1]\n",
    "    rotation_range=20,           # Randomly rotate images within 20 degrees\n",
    "    shear_range=0.2,             # Apply shear transformation\n",
    "    zoom_range=0.2,              # Apply random zoom\n",
    "    horizontal_flip=True,        # Flip images horizontally\n",
    "    vertical_flip=True,           # Flip images vertically\n",
    "    fill_mode='nearest',\n",
    ")\n",
    "\n",
    "# Normalization for the test dataset (no data augmentation for test data)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)  # Rescale pixel values to [0,1] for test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1656 images belonging to 6 classes.\n",
      "Found 1656 images belonging to 6 classes.\n",
      "Found 72 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generating batches of preprocessed images and their labels using flow_from_directory\n",
    "BATCH_SIZE = 32\n",
    "# Training set generator\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    train_dir,                      # Directory containing training images\n",
    "    target_size=(WIDTH, HEIGHT),    # Resizes images to match model input dimensions\n",
    "    batch_size=BATCH_SIZE,          # Size of the batches of data (number of samples per gradient update)\n",
    "    class_mode='categorical'        # Uses categorical labels for multi-class classification\n",
    ")\n",
    "\n",
    "# Validation set generator\n",
    "validation_set = valid_datagen.flow_from_directory(\n",
    "    valid_dir,                      # Directory containing validation images\n",
    "    target_size=(WIDTH, HEIGHT),    # Resizes images to match model input dimensions\n",
    "    batch_size=BATCH_SIZE,          # Size of the batches of data (number of samples per gradient update)\n",
    "    class_mode='categorical'        # Uses categorical labels for multi-class classification\n",
    ")\n",
    "\n",
    "# Test set generator\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    test_dir,                       # Directory containing test images\n",
    "    target_size=(WIDTH, HEIGHT),    # Resizes images to match model input dimensions\n",
    "    batch_size=BATCH_SIZE,          # Size of the batches of data (number of samples per gradient update)\n",
    "    class_mode='categorical',       # Uses categorical labels for multi-class classification\n",
    "    shuffle=False                   # Disables shuffling to maintain order in the test set\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "51/51 [==============================] - 76s 1s/step - loss: 0.2660 - accuracy: 0.9113 - val_loss: 0.1037 - val_accuracy: 0.9657\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 72s 1s/step - loss: 0.0357 - accuracy: 0.9895 - val_loss: 0.0200 - val_accuracy: 0.9963\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 71s 1s/step - loss: 0.0306 - accuracy: 0.9908 - val_loss: 0.0765 - val_accuracy: 0.9694\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 72s 1s/step - loss: 0.0249 - accuracy: 0.9926 - val_loss: 0.0202 - val_accuracy: 0.9945\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 72s 1s/step - loss: 0.0168 - accuracy: 0.9951 - val_loss: 0.0324 - val_accuracy: 0.9896\n"
     ]
    }
   ],
   "source": [
    "# Training the model using the fit method with training and validation data\n",
    "Epochs = 5\n",
    "history = model.fit(\n",
    "    training_set,                                  # Training dataset\n",
    "    epochs=Epochs,                                 # Number of training epochs\n",
    "    steps_per_epoch=training_set.samples // 32,  # Number of steps (batches) per epoch for the training set\n",
    "    validation_data=validation_set,                # Validation dataset\n",
    "    validation_steps=validation_set.samples // 32  # Number of steps (batches) per epoch for the validation set\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 348ms/step - loss: 0.0102 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the trained model using the test dataset\n",
    "\n",
    "scores = model.evaluate(test_set)  # Evaluates the model's performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.010184993967413902\n"
     ]
    }
   ],
   "source": [
    "# Printing the test loss value\n",
    "\n",
    "print('Test Loss: ', scores[0])  # Prints the computed test loss value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Printing the test accuracy\n",
    "\n",
    "print('Test Accuracy: ', scores[1])  # Prints the computed test accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('trained_mobilenetv2_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load an image using PIL library\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def load_image(image_path): # Importing the Image module from the PIL library\n",
    "\n",
    "    \"\"\"\n",
    "    Loads an image using the PIL library.\n",
    "    \n",
    "    Args:\n",
    "    - image_path: The file path of the image to be loaded\n",
    "    \n",
    "    Returns:\n",
    "    - img: The loaded image object\n",
    "    \"\"\"\n",
    "\n",
    "    img = Image.open(image_path)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "def preprocess_image(img):\n",
    "    # Resize the image to the required input size (224x224 for VGG16)\n",
    "    img = img.resize((224, 224))\n",
    "\n",
    "    # Convert the image to RGB if it has a single channel (grayscale)\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "\n",
    "    # Convert PIL Image to numpy array\n",
    "    img_array = img_to_array(img)\n",
    "\n",
    "    # Normalize pixel values to be in the range [0, 1]\n",
    "    img_array = img_array / 255.0\n",
    "\n",
    "    return img_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('trained_mobilenetv2_model.keras')\n",
    "\n",
    "\n",
    "# Use the loaded model to make predictions on new data\n",
    "def detect_defect(image_path):\n",
    "    img = load_image(image_path)  # Load the image (implement this function)\n",
    "    img = preprocess_image(img)   # Preprocess the image (implement this function)\n",
    "\n",
    "    # Make predictions using the loaded model\n",
    "    prediction = loaded_model.predict(np.expand_dims(img, axis=0))\n",
    "\n",
    "    # Check the prediction result\n",
    "    if prediction[0][0] >= 0.5:  # Assuming binary classification (defect or non-defect)\n",
    "        return \"Defect Detected\"\n",
    "    else:\n",
    "        return \"No Defect Detected\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2  # Import OpenCV for image manipulation\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('trained_mobilenetv2_model.keras')\n",
    "\n",
    "# Use the loaded model to make predictions on new data\n",
    "def detect_defect(image_path):\n",
    "    img = load_image(image_path)  # Load the image (implement this function)\n",
    "    img = preprocess_image(img)  # Preprocess the image (implement this function)\n",
    "\n",
    "    # Make predictions using the loaded model\n",
    "    prediction = loaded_model.predict(np.expand_dims(img, axis=0))\n",
    "\n",
    "    # Check the prediction result\n",
    "    if prediction[0][0] >= 0.5:  # Assuming binary classification (defect or non-defect)\n",
    "        result = \"Defect Detected\"\n",
    "    else:\n",
    "        result = \"No Defect Detected\"\n",
    "\n",
    "    # Display the image and the result\n",
    "    plt.imshow(cv2.imread(image_path))  # Show the original image\n",
    "    plt.title(result)\n",
    "    plt.axis('off')  # Hide axis labels\n",
    "    plt.show()\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 664ms/step\n",
      "Detection Result: Defect Detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11008/1847205834.py:27: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "image_path = '/home/topister/Desktop/ML/GroupAssignment1/Transfer Learning/NEU Metal Surface Defects Data/test/Crazing/Cr_1.bmp'  # Replace with your image path\n",
    "detection_result = detect_defect(image_path)\n",
    "print(\"Detection Result:\", detection_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the trained model\n",
    "# model.save('trained_mobilenetv2_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import tensorflow as tf\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2f931b6efc803371f7ed548363c12002c95371f31d41de8b4e58a24f0872b2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
